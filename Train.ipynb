{"cells":[{"cell_type":"markdown","metadata":{"id":"pstlzKwZ2F1l"},"source":["#Import and Initial Mount Disk\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3202,"status":"ok","timestamp":1653033457606,"user":{"displayName":"Vũ Đức Lâm Nguyễn","userId":"05507541945691209538"},"user_tz":-420},"id":"cflxDTZEGl_F","outputId":"6c895b38-9427-4cc1-f86f-eccf64a3cf4a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"]}],"source":["# install library\n","!pip install -U tensorflow-addons"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5411,"status":"ok","timestamp":1653033463012,"user":{"displayName":"Vũ Đức Lâm Nguyễn","userId":"05507541945691209538"},"user_tz":-420},"id":"11TpHNGi2BRj","outputId":"93692bec-be32-4373-f4cf-f6b0dc2a3bdf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","# Mount drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","path = \"/content/drive/My Drive/Colab Notebooks/FaceMaskRecognize\"\n","os.chdir(path)\n","\n","import tensorflow_addons as tfa\n","import tensorflow as tf\n","from tensorflow.keras import models, layers, metrics, optimizers, Model\n","from functools import partial\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","import math\n","import io\n","import pickle\n","import tensorflow_datasets as tfds\n","import random\n","from train.Net import InceptionResNetV1\n","from train.FaceNet import FaceNetModel,call_instance_FaceNet_with_custom, call_instance_FaceNet_without_custom,call_instance_FaceNet_with_last_isDense, convert_train_model_to_embedding\n","from train.Classify import Classify\n","from tool.FormatFunction import FormatFunction\n","from tool.FileFunction import FileFunction\n","from tool.GlobalValue import GlobalValue"]},{"cell_type":"markdown","metadata":{"id":"arWBdNC42LO8"},"source":["# Train"]},{"cell_type":"markdown","metadata":{"id":"cLa5A6udvGwZ"},"source":["## Init value"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":280,"status":"ok","timestamp":1653033471422,"user":{"displayName":"Vũ Đức Lâm Nguyễn","userId":"05507541945691209538"},"user_tz":-420},"id":"-EpkTrUkvL8-"},"outputs":[],"source":["READ_RAW_DATA_THEN_SAVE = False\n","global_value = GlobalValue(image_size=[110,110], batch_size = 512, shuffle_size = 1000, ratio_train = 0.8, ratio_test = 0.1, ratio_valid = 0.1, epochs = 40, small_epochs = 50,\n","                           image_each_class = 15)\n","format_function = FormatFunction(global_value)\n","\n","if READ_RAW_DATA_THEN_SAVE: \n","  label_dict = format_function.get_label_dict(os.path.join(os.getcwd(),\"align_image\"))\n","  path = os.path.join(os.getcwd(),\"src\",\"data\",\"label_dict.pkl\")\n","  with open(path, 'wb') as file:\n","    pickle.dump(label_dict, file)\n","path = os.path.join(os.getcwd(),\"src\",\"data\",\"label_dict.pkl\")\n","with open(path, 'rb') as f:\n","  label_dict = pickle.load(f)\n","\n","file_function = FileFunction()\n"]},{"cell_type":"markdown","metadata":{"id":"j8Rbbjto2TA-"},"source":["## Load data "]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4304,"status":"ok","timestamp":1653033287950,"user":{"displayName":"Vũ Đức Lâm Nguyễn","userId":"05507541945691209538"},"user_tz":-420},"id":"S4p52uj-InOQ"},"outputs":[],"source":["\n","#Save data path to file to read faster\n","if READ_RAW_DATA_THEN_SAVE:\n","  path_image_align  = file_function.get_data_path_by_dictionary(os.path.join(os.getcwd(),\"align_image\"))\n","  path = os.path.join(os.getcwd(),\"src\",\"data\",\"align_data_path.pkl\")\n","  with open(path, 'wb') as file:\n","      pickle.dump(path_image_align, file)\n","\n","  path_image_mask  = file_function.get_data_path_by_dictionary(os.path.join(os.getcwd(),\"face+mask_image\"))\n","  path = os.path.join(os.getcwd(),\"src\",\"data\",\"face+mask_data_path.pkl\")\n","  with open(path, 'wb') as file:\n","      pickle.dump(path_image_mask, file)\n","\n","# Read data path from file\n","path = os.path.join(os.getcwd(),\"src\",\"data\",\"align_data_path.pkl\")\n","with open(path, 'rb') as f:\n","    path_image_align = pickle.load(f)\n","    path_image_align = file_function.get_data_path_with_limit(path_image_align,global_value.IMAGE_EACH_CLASS)\n","path = os.path.join(os.getcwd(),\"src\",\"data\",\"face+mask_data_path.pkl\")\n","with open(path, 'rb') as f:\n","    path_image_mask = pickle.load(f)\n","    path_image_mask = file_function.get_data_path_with_limit(path_image_mask,global_value.IMAGE_EACH_CLASS)\n","# Combine data path\n","\n","path_image_align.extend(path_image_mask)\n","random.shuffle(path_image_align)\n","label_index =list()\n","for path in path_image_align:\n","  label = path.split(\"/\")[-2]\n","  label = label_dict[label]\n","  label_index.append(label)\n","path_dataset = tf.data.Dataset.from_tensor_slices(path_image_align)\n","label_dataset = tf.data.Dataset.from_tensor_slices(label_index)\n","origin_dataset = tf.data.Dataset.zip((path_dataset, label_dataset))"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":443,"status":"ok","timestamp":1653033288391,"user":{"displayName":"Vũ Đức Lâm Nguyễn","userId":"05507541945691209538"},"user_tz":-420},"id":"zYh7mwb-66Ae"},"outputs":[],"source":["# Repeat data and attach label\n","data_set  = origin_dataset.shuffle(global_value.SHUFFLE_SIZE).repeat(2)\n","\n","\n","# read data from path\n","data_set = data_set.map(format_function.process_image, num_parallel_calls=tf.data.AUTOTUNE)\n","# data_set = data_set.filter(lambda image, label: tf.math.not_equal(tf.size(image), 0))\n","data_set = data_set.map(format_function.augment_data, num_parallel_calls=tf.data.AUTOTUNE)\n","data_set = data_set.shuffle(global_value.SHUFFLE_SIZE)\n","\n","# batch data\n","data_set = data_set.batch(global_value.BATCH_SIZE)\n","\n","# # Set cache and prefetch to improve performance\n","data_set = data_set.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"7Dz4PftxE6XC"},"source":["## Start train"]},{"cell_type":"markdown","metadata":{"id":"lZpKIQAUTDpm"},"source":["# Train version 2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QXpeK2-rTN_n","outputId":"c2e59810-7b58-4172-c43a-c4b3668451f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["save_model/face_recognize_entropy49\n","--------------------------big epoch 50--------------------------\n"]}],"source":["# The embedding model\n","input_shape = [global_value.IMAGE_SIZE[0], global_value.IMAGE_SIZE[1], 3]\n","face_net_model = call_instance_FaceNet_with_last_isDense(input_shape, len(label_dict))\n","face_net_model.compile(\n","    optimizer=tf.keras.optimizers.Adam(0.001),\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n","    )\n","#----choose path to save per epoch\n","actual_epochs = 1\n","for i in range(100):\n","  last_save_path = \"save_model/face_recognize_entropy{}\".format(actual_epochs)\n","  if not os.path.exists(last_save_path):\n","    break\n","  actual_epochs += 1\n","\n","# Load save\n","if (actual_epochs != 1):\n","  load_path = \"save_model/face_recognize_entropy{}\".format(actual_epochs-1)\n","  print(load_path)\n","  face_net_model = tf.keras.models.load_model(load_path)\n","\n","\n","\n","# Normal train network\n","for i in range(global_value.EPOCHS):\n","  # Read data path from file\n","  path = os.path.join(os.getcwd(),\"src\",\"data\",\"align_data_path.pkl\")\n","  with open(path, 'rb') as f:\n","      path_image_align = pickle.load(f)\n","      path_image_align = file_function.get_data_path_with_limit(path_image_align,global_value.IMAGE_EACH_CLASS)\n","  path = os.path.join(os.getcwd(),\"src\",\"data\",\"face+mask_data_path.pkl\")\n","  with open(path, 'rb') as f:\n","      path_image_mask = pickle.load(f)\n","      path_image_mask = file_function.get_data_path_with_limit(path_image_mask,global_value.IMAGE_EACH_CLASS)\n","  # Combine data path\n","\n","  path_image_align.extend(path_image_mask)\n","  random.shuffle(path_image_align)\n","  label_index =list()\n","  for path in path_image_align:\n","    label = path.split(\"/\")[-2]\n","    label = label_dict[label]\n","    label_index.append(label)\n","  path_dataset = tf.data.Dataset.from_tensor_slices(path_image_align)\n","  label_dataset = tf.data.Dataset.from_tensor_slices(label_index)\n","  origin_dataset = tf.data.Dataset.zip((path_dataset, label_dataset))\n","  # Repeat data and attach label\n","  data_set  = origin_dataset.shuffle(global_value.SHUFFLE_SIZE).repeat(2)\n","\n","\n","  # read data from path\n","  data_set = data_set.map(format_function.process_image, num_parallel_calls=tf.data.AUTOTUNE)\n","  # data_set = data_set.filter(lambda image, label: tf.math.not_equal(tf.size(image), 0))\n","  data_set = data_set.map(format_function.augment_data, num_parallel_calls=tf.data.AUTOTUNE)\n","  data_set = data_set.shuffle(global_value.SHUFFLE_SIZE)\n","\n","  # batch data\n","  data_set = data_set.batch(global_value.BATCH_SIZE)\n","\n","  # Set cache and prefetch to improve performance\n","  data_set = data_set.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n","  print(\"--------------------------big epoch {}--------------------------\".format(actual_epochs))\n","  history = face_net_model.fit(\n","      data_set,\n","      epochs = 1\n","  )\n","  face_net_model.save(\"save_model/face_recognize_entropy{}\".format(actual_epochs))\n","  with open(\"src/loss/face_recognize_entropy.txt\", \"a\") as file_object:\n","    file_object.write(\"\\n\")\n","    file_object.write(\"epoch {}, loss {}, batch {}\". format(actual_epochs, history.history['loss'], global_value.BATCH_SIZE))\n","  actual_epochs += 1"]},{"cell_type":"markdown","metadata":{"id":"77wRLLpnwljL"},"source":["## Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MaizwhSWwm9D"},"outputs":[],"source":["# face_net_model = tf.keras.models.load_model(\"save_model/align_image_origin36\")\n","# classify = Classify(face_net_model, format_function)\n","# database_embedding = classify.embedding_all_data_by_directory(os.path.join(os.getcwd(),\"dataset\",\"lfw\"))\n","# classify.save_embedding_to_file(database_embedding, os.path.join(os.getcwd(),\"encodings\",\"encode_lfw_epoch36.pkl\"))\n","# #Preprocess data\n","# test_dataset = tf.data.Dataset.list_files(\"dataset/lfw/*/*\",shuffle=False)\n","# test_dataset = test_dataset.map(format_function.get_label_as_number, num_parallel_calls=tf.data.AUTOTUNE)\n","# test_dataset = test_dataset.map(format_function.process_image, num_parallel_calls=tf.data.AUTOTUNE)\n","# test_dataset = test_dataset.filter(lambda image, label: tf.math.not_equal(tf.size(image), 0))\n","# test_dataset = test_dataset.batch(20)\n","\n","# # Accuracy\n","# print(classify.evaluate(test_dataset, database_embedding))"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":40052,"status":"ok","timestamp":1653033560682,"user":{"displayName":"Vũ Đức Lâm Nguyễn","userId":"05507541945691209538"},"user_tz":-420},"id":"lT1WdCieksnL","outputId":"92e57ef5-1536-481a-9b68-ac1269a140d2"},"outputs":[{"data":{"application/javascript":"\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":"download(\"download_fddbdae6-93bf-4289-8660-40bb26817fa7\", \"vecs.tsv\", 4680835)","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":"\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":"download(\"download_b07f5ce8-b98c-4aee-bc3e-e9153f0b36b7\", \"meta.tsv\", 8325)","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Load network\n","face_net_model = tf.keras.models.load_model(\"save_model/face_recognize_entropy47\")\n","face_net_model =  convert_train_model_to_embedding(face_net_model)\n","#Preprocess data\n","test_dataset = tf.data.Dataset.list_files(\"dataset/10_person/*/*\",shuffle=False)\n","test_dataset = test_dataset.map(format_function.get_label_as_number, num_parallel_calls=tf.data.AUTOTUNE)\n","test_dataset = test_dataset.map(format_function.process_image, num_parallel_calls=tf.data.AUTOTUNE)\n","test_dataset = test_dataset.filter(lambda image, label: tf.math.not_equal(tf.size(image), 0))\n","test_dataset = test_dataset.batch(20)\n","# Evaluate the network\n","results = face_net_model.predict(test_dataset)\n","\n","# Save test embeddings for visualization in projector\n","np.savetxt(\"vecs.tsv\", results, delimiter='\\t')\n","\n","out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n","for img, labels in tfds.as_numpy(test_dataset):\n","    [out_m.write(str(x) + \"\\n\") for x in labels]\n","out_m.close()\n","\n","\n","try:\n","  from google.colab import files\n","  files.download('vecs.tsv')\n","  files.download('meta.tsv')\n","except:\n","  pass"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Train.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":0}
